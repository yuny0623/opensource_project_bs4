# opensource_project_bs4
뷰티풀수프4에 개로운 기능 추가. 


기존에 있던 뷰티풀수프4와 새롭게 만든 클래스 및 함수를 추가했습니다.

보완점:
url list에서 뽑아온 모든 문자열리스트에서 데이터를 뽑아내는데, 정확성이 떨어집니다.
단어별로 분류하고, 따로 분배하는 과정이 필요합니다.

naver.com 과 같은 특정 사이트에서 requests의 http요청이 중간에 거절됩니다.
time.sleep()으로 해결보려고 했는데, 되지 않았고, list를 따로 만들어서 요청을 각각 나누어서 보내봤는데,
이 또한 거절됬습니다. 아마 요청 시간과 리스트의 크기와는 별로 상관이 없는 듯 합니다.
--> 즉 되는 사이트가 있고, 안되는 사이트가 있습니다. (원래는 다 되게 해야하는데... )

음... 그리고 딱히 별다른건 없네요... 새로 기능추가할건 아직 잘 모르겠습니다.
제가 여기까지 만들었으니까 이제 다른 분들이 더 추가하는 것만 받고, 좀더 정확성 높이기 위해서 좀만 더
만지면 될 것 같네요.

아 그리고 메인페이지에서 뽑아낸 url들의 텍스트에서 단어를 추출하는 기능이 아직 구현되지 않았습니다.
그건 새롭게 추가해야 할듯 .. !

기존 bs4에 추가된 파일:
access_here.py
collect_text.py
create_obj.py
make_small_list.py
make_tfile.py
url_to_tree.py
z_text.py
이 추가 되었습니다.

access_here.py:
실제로 사용되지 않습니다. 향후에 새로운 기능을 구현할 때 다른 클래스 내부 변수에 쉽게 접근하려고
모든 변수들을 모아놓으려고 한 파일입니다. 지금은 쓰지 않음.
collect_text.py:
메인 페이지에 연결된 모든 url list를 반환합니다. 해당 페이지에 존재하는 문자열의 묶음을 리스트로 반환합니다.
문자열을 정렬하고, url도 정렬합니다.

create_obj.py:
단 한개의 soup객체를 반환하기 위해 만들었습니다. 사용되지 않습니다.

make_small_list.py:
전달받은 url list (분할된 리스트입니다. 분할하지 않으면 무분별한 크롤링으로 인해 서버측에서 requests요청이 거부됨. )를 requests요청을 통해 soup객체를 생성해줍니다. list형식으로 할당하고 리스트를 반환합니다.

make_tfile.py:
텍스트파일로 만들어서 저장해줍니다. 사용되지 않습니다.

url_to_tree.py:
주가 되는 파일입니다. 1layer만큼 들어가서 해당 모든 url들을 리스트로 만들어주고, 찾고 싶은 단어를 찾게 해줍니다. 가장 많은 기능을 수행함.

z_text.py:
테스트파일입니다. 어떻게 동작하는지 어떻게 사용하는지? 명시되어있습니다.
